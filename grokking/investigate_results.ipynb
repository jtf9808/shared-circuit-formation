{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: clean up and add more notes in later sections",
   "id": "debee7c763c121da"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates[\"custom\"] = pio.templates[\"plotly\"]\n",
    "pio.templates[\"custom\"][\"layout\"][\"colorway\"] = px.colors.sequential.RdBu\n",
    "pio.templates.default = \"custom\"\n",
    "\n",
    "\n",
    "from matplotlib.pyplot import ylabel"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = torch.load('output.pt')\n",
    "plt.plot(results['train_losses'])\n",
    "plt.plot(results['test_losses'])\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'test_loss'])"
   ],
   "id": "30f6a8fe852173e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results.keys()",
   "id": "e795f0f65e0123a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = torch.load('output_plus_and_minus.pt', map_location=torch.device('cpu'))\n",
    "plt.plot(results['train_losses'])\n",
    "plt.plot(results['test_losses'])\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'test_loss',])\n"
   ],
   "id": "35c1b4b687dbae6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.keys()\n",
    "plt.plot(results['train_precision_scores'])\n",
    "plt.plot(results['test_precision_scores'])\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'])\n"
   ],
   "id": "c013eccf6bad7e41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['train_losses'])\n",
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['test_losses'])\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['+ train', '- train', '+ test', '- test'])\n"
   ],
   "id": "34920b03eb910477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['train_precisions'])\n",
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['test_precisions'])\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['+ train', '- train', '+ test', '- test'])\n",
    "plt.yscale('log')"
   ],
   "id": "b6ba1805198e6b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from transformer_lens import HookedTransformer, HookedTransformerConfig, HookedEncoderDecoder",
   "id": "a575feb809f2f098",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results['config'].device = 'cpu'",
   "id": "e386abc3edd174ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = HookedTransformer(results['config'])\n",
    "model.load_state_dict(results['model'])"
   ],
   "id": "d2b011f71015a369",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Show model works",
   "id": "35d73d3070993ae8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (1 + 15) % 113\n",
    "model(torch.tensor([1, 114, 15, 113]))[0,3].argmax().item()"
   ],
   "id": "375b5afe74bfbc6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (1 - 15) % 113\n",
    "model(torch.tensor([1, 115, 15, 113]))[0, 3].argmax().item()"
   ],
   "id": "e3a5396e38aefff3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run model on full dataset to look at activations/attention patterns",
   "id": "2d4be49803e03dfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import einops\n",
    "from modular_addition import ModularOperationsDataset\n",
    "operations = (lambda x, y: x + y, lambda x, y: x - y)\n",
    "dataset = ModularOperationsDataset(\n",
    "    base=113,\n",
    "    train_fraction=0.25,\n",
    "    operations=operations,\n",
    ")\n",
    "full_dataset = einops.rearrange(dataset.data, \"i j k -> (i k) j\")\n",
    "plus_dataset = dataset.data[:,:,0]\n",
    "minus_dataset = dataset.data[:,:,1]\n",
    "print(f\"plus: {plus_dataset.shape}, minus: {minus_dataset.shape}, full: {full_dataset.shape}\")"
   ],
   "id": "7aa162b9164db39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output, cache = model.run_with_cache(full_dataset)",
   "id": "4dbe355090135773",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "av_attention = cache[\"pattern\", 0].mean(dim=0).detach().cpu()",
   "id": "721fb5b0fa68f5b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i],\n",
    "        annot=True,            # Annotate each cell with its value\n",
    "        xticklabels=labels,    # Set x-axis labels\n",
    "        yticklabels=labels,    # Set y-axis labels\n",
    "        cmap=\"viridis\",        # Use a color map\n",
    "        cbar=False,            # Disable color bar to reduce clutter\n",
    "        ax=ax,                  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "    ax.set_ylabel('destination token', fontsize=12)\n",
    "    ax.set_xlabel('source token', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "8f476f9bd437b6c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot just attn where = is the destination (all we care about)\n",
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i, -1:],\n",
    "        annot=True,            # Annotate each cell with its value\n",
    "        xticklabels=labels,    # Set x-axis labels\n",
    "        yticklabels=[\"=\"],    # Set y-axis labels\n",
    "        cmap=\"viridis\",        # Use a color map\n",
    "        cbar=False,            # Disable color bar to reduce clutter\n",
    "        ax=ax                  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "    ax.set_ylabel('destination token', fontsize=12)\n",
    "    ax.set_xlabel('source token', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "db56b12a358369b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention patterns for +",
   "id": "791535f5a0b25288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_plus, cache_plus = model.run_with_cache(plus_dataset)\n",
    "av_attention = cache_plus[\"pattern\", 0].mean(dim=0).detach().cpu()\n",
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i],\n",
    "        annot=True,  # Annotate each cell with its value\n",
    "        xticklabels=labels,  # Set x-axis labels\n",
    "        yticklabels=labels,  # Set y-axis labels\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "b9e1a7348a0221f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention patterns for -",
   "id": "b1b2516931c8bf75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_minus, cache_minus = model.run_with_cache(minus_dataset)\n",
    "av_attention = cache_minus[\"pattern\", 0].mean(dim=0).detach().cpu()\n",
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i],\n",
    "        annot=True,  # Annotate each cell with its value\n",
    "        xticklabels=labels,  # Set x-axis labels\n",
    "        yticklabels=labels,  # Set y-axis labels\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "50c6dfbe2391163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we plot the attention patterns for all combinations of a and b. Here we expect this to be very similar to what was found in the original paper with some potential interesting stuff on the openeration as the source attention.",
   "id": "f9d52a0bae2359bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Original model form paper\n",
    "import einops\n",
    "original_results = torch.load('output.pt')\n",
    "original_model = HookedTransformer(original_results['config'])\n",
    "original_model.load_state_dict(original_results['model'])\n",
    "a_s = einops.repeat(torch.arange(113), \"i -> (i j)\", j=113)\n",
    "b_s = einops.repeat(torch.arange(113), \"j -> (i j)\", i=113)\n",
    "equals = einops.repeat(\n",
    "    torch.tensor(113), \" -> (i j)\", i=113, j=113\n",
    ")\n",
    "original_dataset_correctly_ordered = torch.stack([a_s, b_s, equals], dim=1)\n",
    "original_output, original_cache = original_model.run_with_cache(original_dataset_correctly_ordered)\n",
    "a_to_equals_attn_patterns = original_cache[\"pattern\", 0][:,:,-1,0].detach().cpu()\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"attention Patterns for a -> = for original model\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        a_to_equals_attn_patterns[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "f9ff9c8ef356c0a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,0].detach().cpu()\n",
    "a_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,0].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns a -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        a_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        a_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "2f27ff9c1a131f73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "operation_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,1].detach().cpu()\n",
    "operation_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,1].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns operation -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        operation_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        operation_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "dd2046bd9111f825",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "b_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,2].detach().cpu()\n",
    "b_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,2].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns b -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        b_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        b_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "fcda120e716c6797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "equals_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,3].detach().cpu()\n",
    "equals_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,3].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns = -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        equals_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        equals_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "fafbf0930ff3332f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definite periodicity but some interesting stuff happening. Maybe the model is learning a similar alg to the original but with more frequency components to accurately calculate both operations",
   "id": "f5e61090f74a9dec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check the embedding matrix",
   "id": "aac5d9ad89f86075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "W_E = model.embed.W_E.cpu().detach()[:113]\n",
    "sns.heatmap(\n",
    "        W_E.numpy(),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "    )"
   ],
   "id": "b3800d4a2b965d67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "plt.plot(S)\n",
    "plt.title('W_E singular values')"
   ],
   "id": "bb7d3a00ea179c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Create heatmap\n",
    "px.imshow(U)"
   ],
   "id": "4d90564cc47521e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "px.line(U[:, :8].T, title=\"Principle Components of Embedding\").update_layout(\n",
    "    xaxis_title=\"Input Vocabulary\")"
   ],
   "id": "2e2d74c73b7bb9a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fourier_basis = []\n",
    "fourier_basis_names = []\n",
    "fourier_basis.append(torch.ones(113))\n",
    "fourier_basis_names.append('Constant')\n",
    "for freq in range(1, 113//2 + 1):\n",
    "    fourier_basis.append(torch.sin(torch.arange(113)*2 * torch.pi*freq /113))\n",
    "    fourier_basis_names.append(f'Sin {freq}')\n",
    "    fourier_basis.append(torch.cos(torch.arange(113)*2 * torch.pi*freq /113))\n",
    "    fourier_basis_names.append(f'Cos {freq}')\n",
    "fourier_basis = torch.stack(fourier_basis, dim=0)\n",
    "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1)\n",
    "px.imshow(fourier_basis, y=fourier_basis_names,color_continuous_scale='RdBu',).update_layout(xaxis_title=\"Input\", yaxis_title=\"Cofourier_basis_namesmponent\")"
   ],
   "id": "d4a44fdcaf8627b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ fourier_basis.T)",
   "id": "427ff60aa421b7fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ W_E, color_continuous_scale='RdBu', y =fourier_basis_names, title='embedding_in_fourier_basis').update_layout(xaxis_title=\"Residual Stream\", yaxis_title=\"fourier_component\")",
   "id": "3d71ff25c8a80e4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(y=(fourier_basis @ W_E).norm(dim=-1), x=fourier_basis_names, title='embedding_in_fourier_basis').update_layout(xaxis_title=\"Residual Stream\", yaxis_title=\"fourier_component\")",
   "id": "f20289976803ac1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(fourier_basis @ W_E).norm(dim=-1)",
   "id": "55c8b26490695f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(fourier_basis[[32, 48, 100]].mean(0))\n",
   "id": "a23d7e04e6e4f002",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "key_freq_indicies = [31, 32, 47, 48, 99, 100]\n",
    "key_fourier_embed = (fourier_basis @ W_E)[key_freq_indicies]\n",
    "px.imshow(key_fourier_embed@key_fourier_embed.T,color_continuous_scale='RdBu',color_continuous_midpoint=0)"
   ],
   "id": "167e6e7f05d4afe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Look at frequencies in the mlp hidden activations",
   "id": "3fac27929ca8ecf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neuron_acts_plus = cache_plus[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_pre_acts_plus = cache_plus[\"pre\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_acts_minus = cache_minus[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_pre_acts_minus = cache_minus[\"pre\", 0, \"mlp\"][:, -1, :]"
   ],
   "id": "3200d749ead85d16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "neuron_acts_plus.shape",
   "id": "4b119c62a1c2c388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_plus[:, 1].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "3bcd7abc3240fbdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_minus[:, 1].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "11cf87ecb59bd0fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ neuron_acts_plus[:, 1].reshape(113, 113) @ fourier_basis.T, color_continuous_scale='RdBu', color_continuous_midpoint=0, title='2d transform of neuron 1 for plus op', x=fourier_basis_names, y=fourier_basis_names).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "6e23434d5b0decc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ neuron_acts_minus[:, 1].reshape(113, 113) @ fourier_basis.T, color_continuous_scale='RdBu', title='2d transform of neuron 1 for minus op', x=fourier_basis_names, y=fourier_basis_names, color_continuous_midpoint=0).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")\n",
   "id": "8aa38d8e77acf782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### neuron clusters",
   "id": "d8885832cb151a3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fourier_neuron_acts_plus = fourier_basis @ einops.rearrange(neuron_acts_plus, \"(a b) neuron -> neuron a b\", a=113, b=113) @ fourier_basis.T\n",
    "# Center these by removing the mean - doesn't matter!\n",
    "fourier_neuron_acts_plus[:, 0, 0] = 0.\n",
    "print(\"fourier_neuron_acts\", fourier_neuron_acts_plus.shape)"
   ],
   "id": "30ae026847c114d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neuron_freq_norm_plus = torch.zeros(113//2, model.cfg.d_mlp)\n",
    "for freq in range(0, 113//2):\n",
    "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "            neuron_freq_norm_plus[freq] += fourier_neuron_acts_plus[:, x, y]**2\n",
    "neuron_freq_norm_plus = neuron_freq_norm_plus / fourier_neuron_acts_plus.pow(2).sum(dim=[-1, -2])[None, :]\n",
    "px.imshow(neuron_freq_norm_plus, y=torch.arange(1, 113//2+1), title=\"Neuron Frac Explained by Freq plus\", color_continuous_scale='RdBu', color_continuous_midpoint=0, aspect=\"auto\",).update_layout(xaxis_title=\"Neuron\", yaxis_title=\"Freq\")"
   ],
   "id": "f89486ebc9c69b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(neuron_freq_norm_plus.max(dim=0).values.sort().values, title=\"Max Neuron Frac Explained over Freqs plus\").update_layout(xaxis_title=\"Neuron\")",
   "id": "ed10053f7805191c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fourier_neuron_acts_minus = fourier_basis @ einops.rearrange(neuron_acts_minus, \"(a b) neuron -> neuron a b\", a=113, b=113) @ fourier_basis.T\n",
    "# Center these by removing the mean - doesn't matter!\n",
    "fourier_neuron_acts_minus[:, 0, 0] = 0.\n",
    "print(\"fourier_neuron_acts\", fourier_neuron_acts_minus.shape)"
   ],
   "id": "22c5d09f580817d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neuron_freq_norm_minus = torch.zeros(113//2, model.cfg.d_mlp)\n",
    "for freq in range(0, 113//2):\n",
    "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "            neuron_freq_norm_minus[freq] += fourier_neuron_acts_minus[:, x, y]**2\n",
    "neuron_freq_norm_minus = neuron_freq_norm_minus / fourier_neuron_acts_minus.pow(2).sum(dim=[-1, -2])[None, :]\n",
    "px.imshow(neuron_freq_norm_minus, y=torch.arange(1, 113//2+1), title=\"Neuron Frac Explained by Freq plus\", color_continuous_scale='RdBu', color_continuous_midpoint=0, aspect=\"auto\",).update_layout(xaxis_title=\"Neuron\", yaxis_title=\"Freq\")"
   ],
   "id": "3b29a6632176c08d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(neuron_freq_norm_minus.max(dim=0).values.sort().values, title=\"Max Neuron Frac Explained over Freqs plus\").update_layout(xaxis_title=\"Neuron\")\n",
   "id": "de5a6c1abde0f88a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TODO: Maybe try to combine the above?",
   "id": "fcff5a73bc45f7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Look at Unembedding",
   "id": "12353d42bebd046d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "W_U = model.unembed.W_U.cpu().detach()\n",
    "W_O = model.blocks[0].mlp.W_out.cpu().detach()"
   ],
   "id": "c970ccb5012244c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "20b0e814ba8849f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "W_OU = W_O @ W_U",
   "id": "93a0bd220dcb1168",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(W_OU, color_continuous_scale='RdBu', color_continuous_midpoint=0, aspect=\"auto\", height=800, width=800)",
   "id": "1646fd459934907",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis@W_OU.T, y=fourier_basis_names,color_continuous_scale='RdBu', color_continuous_midpoint=0, title=\"frequency componenets of W_OU\").update_layout(xaxis_title=\"output\", yaxis_title=\"Freq\")",
   "id": "e7d0d53c8a3f71f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(y=(fourier_basis@W_OU.T).T.norm(dim=0), x=fourier_basis_names,title=\"normed frequency componenets of W_OU\").update_layout(xaxis_title=\"output\", yaxis_title=\"Freq\")",
   "id": "51ae72dff1208312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "## show that corresponding the frequency component of W_OU output for the neuron activations with a frequency 16 is also 16 (this is for the final cos(w) output in the the trig addition)",
   "id": "b73c21947d4c3904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neurons_16 = neuron_freq_norm_plus[16-1] > 0.7\n",
    "px.line(y=(fourier_basis@W_OU[neurons_16].T).T.norm(dim=0), x=fourier_basis_names,title=\"normed frequency componenets of W_OU\").update_layout(xaxis_title=\"output\", yaxis_title=\"Freq\")\n"
   ],
   "id": "f3e53319d738759b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Looking at plus and minus",
   "id": "3edb449142e38bb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_plus[:, 2].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0, title = 'plus operation neuron activations [neuron 2]').update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "956b7ac10b2b397c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_minus[:, 2].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0, title = 'minus operation neuron activations [neuron 2]').update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "9ed9b9ca52ce6f45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3adc97dc04374d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.subplots as sp\n",
    "import numpy as np\n",
    "max_plots = 25\n",
    "\n",
    "num_plots = min(max_plots, neuron_acts_plus.shape[1])\n",
    "grid_size = int(2)\n",
    "\n",
    "fig = sp.make_subplots(rows=num_plots, cols=2, subplot_titles=[j for i in range(num_plots) for j in [f\"Plus Neuron {i}\", f\"Minus Neuron {i}\"]])\n",
    "\n",
    "for i in range(num_plots):\n",
    "    reshaped_data_plus = neuron_acts_plus[:, i].reshape(113, 113)\n",
    "    max_val_plus = reshaped_data_plus.max()\n",
    "    reshaped_data_minus = neuron_acts_minus[:, i].reshape(113, 113)\n",
    "    max_val_minus = reshaped_data_minus.max()\n",
    "    overall_max = max([max_val_plus, max_val_minus])\n",
    "    normalized_data_plus = reshaped_data_plus / (overall_max + 1e-8)\n",
    "    normalized_data_minus = reshaped_data_minus / (overall_max + 1e-8)\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_plus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=1)\n",
    "\n",
    "\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_minus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500 * max_plots, width=grid_size * 500,\n",
    "    title_text=\"Neuron Activations\",\n",
    "    coloraxis=dict(cmid=0, colorscale='RdBu')\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ],
   "id": "18a655b52992df95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5e743fae39a61df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plus_not_minus = []\n",
    "minus_not_plus = []\n",
    "for i in range(512):\n",
    "    if neuron_acts_minus[:, i].sum()/neuron_acts_plus[:, i].sum() < 0.1:\n",
    "        plus_not_minus.append(i)\n",
    "    if neuron_acts_plus[:, i].sum()/neuron_acts_minus[:, i].sum() < 0.1:\n",
    "        minus_not_plus.append(i)\n",
    "\n",
    "\n",
    "all_ = plus_not_minus + minus_not_plus\n",
    "num_plots = len(all_)\n",
    "grid_size = int(2)\n",
    "\n",
    "fig = sp.make_subplots(rows=num_plots, cols=2, subplot_titles=[j for i in range(num_plots) for j in [f\"Plus Neuron {i}\", f\"Minus Neuron {i}\"]])\n",
    "\n",
    "for i, neuron_index in enumerate(all_):\n",
    "    reshaped_data_plus = neuron_acts_plus[:, neuron_index].reshape(113, 113)\n",
    "    max_val_plus = reshaped_data_plus.max()\n",
    "    reshaped_data_minus = neuron_acts_minus[:, neuron_index].reshape(113, 113)\n",
    "    max_val_minus = reshaped_data_minus.max()\n",
    "    overall_max = max([max_val_plus, max_val_minus])\n",
    "    normalized_data_plus = reshaped_data_plus / (overall_max + 1e-8)\n",
    "    normalized_data_minus = reshaped_data_minus / (overall_max + 1e-8)\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_plus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=1)\n",
    "\n",
    "\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_minus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500 * max_plots, width=grid_size * 500,\n",
    "    title_text=\"Neuron Activations\",\n",
    "    coloraxis=dict(cmid=0, colorscale='RdBu')\n",
    ")\n",
    "fig.show()"
   ],
   "id": "7e7e76e44c5e3a7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ablating neurons that only fire for one operation",
   "id": "6fff5dfe48dc6d9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:20:03.069527Z",
     "start_time": "2025-03-17T09:20:01.746642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_lens import utils\n",
    "def plus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, plus_not_minus] = 0.\n",
    "    return value\n",
    "\n",
    "plus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "plus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "def minus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    value[:, :, minus_not_plus] = 0.\n",
    "    return value\n",
    "\n",
    "minus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "minus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )"
   ],
   "id": "227081cfd328d443",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the value tensor: torch.Size([12769, 4, 512])\n",
      "Shape of the value tensor: torch.Size([12769, 4, 512])\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:20:03.112203Z",
     "start_time": "2025-03-17T09:20:03.078812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score\n",
    "plus_dataset_labels = (plus_dataset[:,0] + plus_dataset[:,2]) % 113\n",
    "minus_dataset_labels = (minus_dataset[:,0] - minus_dataset[:,2]) % 113\n",
    "\n",
    "plus_neurons_ablated_ablated_plus_predictions = plus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "plus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, plus_neurons_ablated_ablated_plus_predictions, average='macro')\n",
    "\n",
    "plus_neurons_ablated_ablated_minus_predictions = plus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "plus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, plus_neurons_ablated_ablated_minus_predictions, average='macro')\n",
    "\n",
    "minus_neurons_ablated_ablated_plus_predictions = minus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "minus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, minus_neurons_ablated_ablated_plus_predictions, average='macro')\n",
    "\n",
    "minus_neurons_ablated_ablated_minus_predictions = minus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "minus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, minus_neurons_ablated_ablated_minus_predictions, average='macro')\n",
    "\n",
    "\n",
    "print(f'after ablating all but neurons only active in plus: plus precision {plus_neurons_ablated_ablated_plus_precision} minus precision {plus_neurons_ablated_ablated_minus_precision}')\n",
    "print(f'after ablating all but neurons only active in minus: plus precision {minus_neurons_ablated_ablated_plus_precision} minus precision {minus_neurons_ablated_ablated_minus_precision}')"
   ],
   "id": "98182ff86cef6a50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after ablating all but neurons only active in plus: plus precision 0.22997348393722042 minus precision 0.9917927362577618\n",
      "after ablating all but neurons only active in minus: plus precision 0.9990725177699925 minus precision 0.3939233817539317\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:22:14.241018Z",
     "start_time": "2025-03-17T09:22:12.486931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def all_but_plus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    value[:, :, [i for i in range(512) if i not in plus_not_minus]] = 0.\n",
    "    return value\n",
    "\n",
    "all_but_plus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "all_but_plus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "def all_but_minus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    value[:, :, [i for i in range(512) if i not in minus_not_plus]] = 0.\n",
    "    return value\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n"
   ],
   "id": "96eba0d148ec978a",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:24:48.996124Z",
     "start_time": "2025-03-17T09:24:48.951939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_but_plus_neurons_ablated_ablated_plus_predictions = all_but_plus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_plus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, all_but_plus_neurons_ablated_ablated_plus_predictions, average='macro', zero_division=0)\n",
    "\n",
    "all_but_plus_neurons_ablated_ablated_minus_predictions = all_but_plus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_plus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, all_but_plus_neurons_ablated_ablated_minus_predictions, average='macro', zero_division=0)\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_plus_predictions = all_but_minus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_minus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, all_but_minus_neurons_ablated_ablated_plus_predictions, average='macro', zero_division=0)\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_minus_predictions = all_but_minus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_minus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, all_but_minus_neurons_ablated_ablated_minus_predictions, average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "print(f'after ablating all but neurons only active in plus: plus precision {all_but_plus_neurons_ablated_ablated_plus_precision} minus precision {all_but_plus_neurons_ablated_ablated_minus_precision}')\n",
    "print(f'after ablating all but neurons only active in minus: plus precision {all_but_minus_neurons_ablated_ablated_plus_precision} minus precision {all_but_minus_neurons_ablated_ablated_minus_precision}')"
   ],
   "id": "97c87c741d138ef4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after ablating all but neurons only active in plus: plus precision 0.00806736360449042 minus precision 0.0005453249533054084\n",
      "after ablating all but neurons only active in minus: plus precision 0.0011633032147423442 minus precision 0.014751908804845129\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:25:00.074419Z",
     "start_time": "2025-03-17T09:25:00.071426Z"
    }
   },
   "cell_type": "code",
   "source": "# do some recursive albation studies",
   "id": "408abaa9f3243e45",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T10:29:28.653514Z",
     "start_time": "2025-03-17T10:27:22.480877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "plus_results = model(plus_dataset)\n",
    "plus_predictions = plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "plus_precision = precision_score(plus_dataset_labels, plus_predictions, average='macro', zero_division=0)\n",
    "print(plus_precision)\n",
    "plus_ablate_list = []\n",
    "ablation_precisions = []\n",
    "tau = 0.001 #taking inspiration from https://arxiv.org/pdf/2304.14997\n",
    "current_precision_score = plus_precision\n",
    "for i in tqdm(range(512)):\n",
    "    new_ablate_list = plus_ablate_list.copy()\n",
    "    new_ablate_list.append(i)\n",
    "    def ablation_hook(\n",
    "        value,\n",
    "        hook\n",
    "    ):\n",
    "        value[:, :, new_ablate_list] = 0.\n",
    "        return value\n",
    "\n",
    "    ablated_plus_results = model.run_with_hooks(\n",
    "        plus_dataset,\n",
    "        fwd_hooks=[(\n",
    "            utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "            ablation_hook\n",
    "            )]\n",
    "        )\n",
    "    ablated_plus_predictions = ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "    ablated_plus_precision = precision_score(plus_dataset_labels, ablated_plus_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    if current_precision_score - ablated_plus_precision < tau:\n",
    "        current_precision_score = ablated_plus_precision\n",
    "        plus_ablate_list = new_ablate_list.copy()\n",
    "        ablation_precisions.append(ablated_plus_precision)\n",
    "# print(new_ablate_list)\n",
    "print([i for i in range(512) if i not in plus_ablate_list])\n",
    "print(ablation_precisions)"
   ],
   "id": "53d590fa3488f245",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [02:05<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 70, 80, 88, 92, 102, 103, 126, 142, 147, 150, 167, 169, 172, 173, 178, 179, 180, 190, 196, 199, 202, 204, 213, 214, 222, 235, 237, 239, 246, 247, 256, 261, 265, 271, 273, 274, 277, 282, 288, 289, 290, 300, 315, 318, 319, 321, 322, 325, 329, 331, 337, 340, 341, 343, 347, 350, 351, 352, 353, 355, 356, 357, 359, 360, 361, 370, 372, 373, 376, 377, 378, 379, 384, 389, 395, 399, 402, 404, 405, 407, 410, 411, 416, 420, 421, 422, 423, 426, 429, 430, 431, 432, 434, 438, 442, 443, 446, 448, 452, 454, 455, 456, 457, 466, 468, 469, 472, 473, 479, 481, 491, 502, 503, 511]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9998460946517892, 0.9998460946517892, 0.9998460946517892, 0.9998460946517892, 0.9993080660415018, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9993013507219376, 0.9987606569328285, 0.9987606569328285, 0.99868437928218, 0.9987606569328285, 0.9987606569328285, 0.9988369345834768, 0.9988369345834768, 0.9988369345834768, 0.9987593068859143, 0.9988369345834768, 0.9988369345834768, 0.9988369345834768, 0.9982968689555003, 0.9982968689555003, 0.9985290889720486, 0.9986067166696112, 0.9984507743037112, 0.9982995690493286, 0.9983035156387273, 0.9983035156387273, 0.9983035156387273, 0.9983035156387273, 0.9983035156387273, 0.9984574209869381, 0.9984574209869381, 0.9983797932893756, 0.9983797932893756, 0.9983797932893756, 0.9983797932893756, 0.9992277382501111, 0.9983200250251991, 0.9983200250251991, 0.9982364137205489, 0.9983910409545412, 0.9984700186990176, 0.9984700186990176, 0.9983134132569785, 0.9983134132569785, 0.9983923910014553, 0.9983923910014553, 0.9983923910014553, 0.9983923910014553, 0.9984700186990176, 0.9984700186990176, 0.9984700186990176, 0.998846315081863, 0.998846315081863, 0.998846315081863, 0.998692409733652, 0.9987700374312146, 0.998843649903041, 0.9987646482639289, 0.9987646482639289, 0.9987646482639289, 0.9987646482639289, 0.998383784620476, 0.998383784620476, 0.998383784620476, 0.9981542885923916, 0.9981542885923916, 0.9981542885923916, 0.9981542885923916, 0.9977668776160764, 0.9977668776160764, 0.9977668776160764, 0.9977668776160764, 0.9977668776160764, 0.9978465778181433, 0.9978465778181433, 0.9978465778181433, 0.9981493576723596, 0.9980022097915954, 0.9976154210306796, 0.9976154210306796, 0.9976154210306796, 0.9976154210306796, 0.9976154210306796, 0.9976154210306796, 0.9976154210306796, 0.9976154210306796, 0.997463494199498, 0.9973845164550215, 0.9966493125464468, 0.9966493125464468, 0.99709759717972, 0.99709759717972, 0.99709759717972, 0.99709759717972, 0.99709759717972, 0.99709759717972, 0.99709759717972, 0.99709759717972, 0.9971821004593103, 0.9970295102430072, 0.9971044727617479, 0.9971044727617479, 0.9971044727617479, 0.9971044727617479, 0.9971044727617479, 0.9971044727617479, 0.9962625072860138, 0.9962625072860138, 0.9963500089153616, 0.9960422104862753, 0.9965656593783376, 0.9965656593783376, 0.995833064016011, 0.9960044268430391, 0.9959267991454767, 0.9958491714479142, 0.9952626807186299, 0.9952626807186299, 0.9952626807186299, 0.9952626807186299, 0.9952613306717157, 0.9952613306717157, 0.9952639958505377, 0.9952639958505377, 0.9952639958505377, 0.9952639958505377, 0.9951100905023269, 0.9950324628047642, 0.9950324628047642, 0.9951087404554128, 0.9950070068255137, 0.9950070068255137, 0.9950070068255137, 0.9945584690289464, 0.9944808413313839, 0.9944808413313839, 0.9945584690289464, 0.9941655408034522, 0.9941655408034522, 0.9941655408034522, 0.9941655408034522, 0.9941655408034522, 0.9936320749963785, 0.9936320749963785, 0.9936320749963785, 0.9936320749963785, 0.9936320749963785, 0.9936320749963785, 0.9930390796352657, 0.9933678837219193, 0.9933678837219193, 0.9933678837219193, 0.9933678837219193, 0.9933678837219193, 0.9934455114194818, 0.9934455114194818, 0.9934455114194818, 0.9935257472978642, 0.9936777430743655, 0.9936777430743655, 0.9936777430743655, 0.9936777430743655, 0.9936777430743655, 0.9934358744387051, 0.9932022098516036, 0.9932022098516036, 0.9932818277045308, 0.9932818277045308, 0.9932818277045308, 0.9940557788283344, 0.9942110219561241, 0.9942110219561241, 0.9942110219561241, 0.9942110219561241, 0.9942110219561241, 0.9941367464081583, 0.9941367464081583, 0.9941349503183564, 0.9945204475219827, 0.9941396638653474, 0.9941396638653474, 0.9940633742673812, 0.9940633742673812, 0.9940603755625248, 0.9937623353473573, 0.9937623353473573, 0.9937648720980516, 0.9937648720980516, 0.9931738597279031, 0.9931738597279031, 0.9931738597279031, 0.9931738597279031, 0.9931738597279031, 0.9931738597279031, 0.9931738597279031, 0.992426129986756, 0.992426129986756, 0.9924185939445084, 0.9920413836542691, 0.9912350414480143, 0.9912350414480143, 0.9918570041849866, 0.9915943568237692, 0.9914402470458898, 0.9907422615926798, 0.9907422615926798, 0.9907422615926798, 0.9907422615926798, 0.9907422615926798, 0.9907422615926798, 0.9907422615926798, 0.9907422615926798, 0.9916572475213192, 0.991580969870671, 0.9915092559290561, 0.9915092559290561, 0.9915092559290561, 0.9915092559290561, 0.9915092559290561, 0.9915092559290561, 0.9915092559290561, 0.9909193277687987, 0.9909193277687987, 0.9909193277687987, 0.9911574566621748, 0.9906940238390386, 0.9906203427308747, 0.9906203427308747, 0.9906203427308747, 0.9906203427308747, 0.989943160057714, 0.9900168411658778, 0.9900632214832726, 0.9900632214832726, 0.989922714962004, 0.989922714962004, 0.989922714962004, 0.989922714962004, 0.9905675981687398, 0.9905675981687398, 0.990131988913173, 0.990131988913173, 0.9900583078050091, 0.9890977126282224, 0.9890977126282224, 0.9890977126282224, 0.9890977126282224, 0.9895962642526889, 0.9886091373259023, 0.9889161302144059, 0.9889161302144059, 0.9889161302144059, 0.9889159947329929, 0.9887618631019006, 0.988172200120119, 0.988172200120119, 0.9900759714825574, 0.9896297327712327, 0.9896297327712327, 0.9896297327712327, 0.9893744484691446, 0.9893744484691446, 0.9893086328701232, 0.9893086328701232, 0.9893086328701232, 0.9888621055704484, 0.9884134583436408, 0.9882709723846466, 0.9882709723846466, 0.9881189888754806, 0.9904504188978432, 0.9906046497462851, 0.9899918166074071, 0.990141189610017, 0.9896911442270064, 0.9896911442270064, 0.9892347323616507, 0.9892347323616507, 0.9892347323616507, 0.98827449403894, 0.98827449403894, 0.98827449403894, 0.98827449403894, 0.98827449403894, 0.9894877032205961, 0.9894877032205961, 0.9894877032205961, 0.9885653680246282, 0.9897704565532855, 0.9896204830639309, 0.9896204830639309, 0.9896204830639309, 0.9886867890948041, 0.9886867890948041, 0.9886867890948041, 0.9886867890948041, 0.9880013494152554, 0.9880013494152554, 0.9880013494152554, 0.9880770237709912, 0.9880770237709912, 0.9880770237709912, 0.9880013494152554, 0.9885192371164382, 0.9885192371164382, 0.9885192371164382, 0.9882202633003809, 0.9888558437187419, 0.9888558437187419, 0.9882836998727733, 0.9883620268084459, 0.9883620268084459, 0.9883620268084459, 0.9883620268084459, 0.9883620268084459, 0.9881311096564702, 0.9881311096564702, 0.9881311096564702, 0.9878468476478528, 0.9892074205443122, 0.9894260481361278, 0.9894984804119499, 0.9894984804119499, 0.9893425380460501, 0.9893425380460501, 0.9893425380460501, 0.989546272405582, 0.989546272405582, 0.989546272405582, 0.989546272405582, 0.989546272405582, 0.989546272405582, 0.989546272405582, 0.9894670811067041, 0.9894670811067041, 0.9890055001283248, 0.9890055001283248, 0.9890055001283248, 0.9890055001283248, 0.9890055001283248, 0.9890055001283248, 0.9890055001283248, 0.9890055001283248, 0.9890055001283248, 0.988301601291607, 0.988301601291607, 0.988301601291607, 0.988301601291607, 0.9881406470377323, 0.9881406470377323, 0.9880736877717959, 0.9880736877717959, 0.9879197475085787, 0.9878531074979162, 0.9878531074979162, 0.9878531074979162, 0.9878531074979162, 0.9911728506215557, 0.9929694986500665, 0.9929694986500665, 0.9929694986500665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T10:50:55.423521Z",
     "start_time": "2025-03-17T10:49:16.112871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "minus_results = model(minus_dataset)\n",
    "minus_predictions = minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "minus_precision = precision_score(minus_dataset_labels, minus_predictions, average='macro', zero_division=0)\n",
    "print(minus_precision)\n",
    "minus_ablate_list = []\n",
    "ablation_precisions = []\n",
    "tau = 0.001 #taking inspiration from https://arxiv.org/pdf/2304.14997\n",
    "current_precision_score = minus_precision\n",
    "for i in tqdm(range(512)):\n",
    "    new_ablate_list = minus_ablate_list.copy()\n",
    "    new_ablate_list.append(i)\n",
    "    def ablation_hook(\n",
    "        value,\n",
    "        hook\n",
    "    ):\n",
    "        value[:, :, new_ablate_list] = 0.\n",
    "        return value\n",
    "\n",
    "    ablated_minus_results = model.run_with_hooks(\n",
    "        minus_dataset,\n",
    "        fwd_hooks=[(\n",
    "            utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "            ablation_hook\n",
    "            )]\n",
    "        )\n",
    "    ablated_minus_predictions = ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "    ablated_minus_precision = precision_score(minus_dataset_labels, ablated_minus_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    if current_precision_score - ablated_minus_precision < tau:\n",
    "        current_precision_score = ablated_minus_precision\n",
    "        minus_ablate_list = new_ablate_list.copy()\n",
    "        ablation_precisions.append(ablated_minus_precision)\n",
    "    else:\n",
    "        print(ablated_minus_precision)\n",
    "# print(new_ablate_list)\n",
    "print([i for i in range(512) if i not in minus_ablate_list])\n",
    "print(ablation_precisions)"
   ],
   "id": "af84263d36c42d5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 24/512 [00:04<01:28,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998525049851677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 43/512 [00:08<01:23,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998214539061427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 69/512 [00:13<01:14,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9943772160497613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 77/512 [00:14<01:14,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736336810278147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 82/512 [00:15<01:09,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965883358694502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 104/512 [00:19<01:12,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733822142759232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 108/512 [00:20<01:23,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9121102340899193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 135/512 [00:25<01:10,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9939456301226742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 140/512 [00:26<01:06,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9905705307158306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 143/512 [00:26<01:23,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993148639918722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 148/512 [00:27<01:07,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9883618184063723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 151/512 [00:28<01:14,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9916543557896275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 156/512 [00:29<01:14,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994669815386399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 161/512 [00:30<01:10,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989823929867812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 165/512 [00:31<01:04,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9944827272918468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 170/512 [00:32<01:13,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994559942443176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 174/512 [00:33<01:29,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9640620090459026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 178/512 [00:34<01:06,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94238062612308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 180/512 [00:34<01:08,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946340740779921\n",
      "0.9679391560824709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 181/512 [00:34<01:05,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9939113377875534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 189/512 [00:36<01:05,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9926205308902668\n",
      "0.9900614228363243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 192/512 [00:37<01:03,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9944736246270336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 201/512 [00:38<00:57,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9795372802791011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 203/512 [00:39<00:56,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9894146707584707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 210/512 [00:40<00:54,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9940805978278783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 215/512 [00:41<00:52,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925391140750851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 224/512 [00:43<01:05,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901395391963205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 227/512 [00:44<00:57,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993227714080746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 241/512 [00:46<00:51,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9936229028897429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 247/512 [00:48<00:49,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9912099248793204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 258/512 [00:49<00:44,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900037052415697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 266/512 [00:51<00:42,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933732314689833\n",
      "0.989200395167484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 269/512 [00:51<00:42,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933228316016978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 273/512 [00:52<00:42,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9882479126156737\n",
      "0.9658504208693399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 275/512 [00:52<00:41,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9911735704873077\n",
      "0.9925609456806844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 283/512 [00:54<00:42,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9931575436437355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 290/512 [00:55<00:43,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917900934770228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 310/512 [00:59<00:40,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9262626813732698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 312/512 [01:00<00:38,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9943919610765858\n",
      "0.9876328701914587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 317/512 [01:01<00:34,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950710363869859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 321/512 [01:01<00:33,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9899584475772085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 323/512 [01:02<00:34,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9169641288362202\n",
      "0.9940089103934466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 327/512 [01:03<00:32,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9946896582594777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 331/512 [01:03<00:31,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9936401306646575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 342/512 [01:05<00:29,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9859958420021715\n",
      "0.9924763563791875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 345/512 [01:06<00:28,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933146000729767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 352/512 [01:07<00:27,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987808710316845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 354/512 [01:07<00:27,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9731061254910627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 356/512 [01:08<00:27,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9921078939925774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 358/512 [01:08<00:26,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9907021518252418\n",
      "0.9393908716852452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 361/512 [01:08<00:26,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880598383581076\n",
      "0.9643651526021855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 371/512 [01:10<00:24,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989495362481493\n",
      "0.9885455349154368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 374/512 [01:11<00:23,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8752440208754908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 376/512 [01:11<00:24,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9860587981933796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 379/512 [01:12<00:25,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9421324619989702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 382/512 [01:12<00:25,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9891285575270264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 384/512 [01:13<00:25,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9857111036315651\n",
      "0.9876106487831414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 387/512 [01:13<00:27,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9637021320705218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 390/512 [01:14<00:28,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9851736103752237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 397/512 [01:15<00:21,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983264239184925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 400/512 [01:16<00:23,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914586026381462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 404/512 [01:17<00:21,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92180723290519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 407/512 [01:17<00:20,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9887174321506416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 409/512 [01:18<00:19,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9864488911320661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 411/512 [01:18<00:19,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880679409895955\n",
      "0.9813969523018704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 412/512 [01:18<00:19,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9895287700873323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 417/512 [01:19<00:17,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9894252604113696\n",
      "0.9270034573509356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 420/512 [01:20<00:16,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9870564169679092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 422/512 [01:20<00:17,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987264707163621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 424/512 [01:21<00:16,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983906969847774\n",
      "0.9889435126783306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 428/512 [01:22<00:16,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9178256310202073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 431/512 [01:22<00:16,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9886559551893671\n",
      "0.982968668895471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 432/512 [01:22<00:15,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9884298525362788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 434/512 [01:23<00:15,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956359125167469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 436/512 [01:23<00:13,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9890676210466663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 439/512 [01:24<00:13,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9868638618173085\n",
      "0.9828597773150564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 441/512 [01:24<00:14,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9790681270991047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 442/512 [01:24<00:16,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9869001384561479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 443/512 [01:25<00:16,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9684854473093361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 448/512 [01:26<00:13,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9885260652300324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 453/512 [01:27<00:12,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9822070480176583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 456/512 [01:27<00:12,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9870231048593315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 457/512 [01:28<00:12,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9870782518457316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 458/512 [01:28<00:12,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9335400898802956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 459/512 [01:28<00:13,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9706974786711727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 465/512 [01:30<00:10,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9870379905767451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 468/512 [01:30<00:09,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756946331461164\n",
      "0.9874861487278406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 469/512 [01:31<00:09,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9681822689516152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 470/512 [01:31<00:09,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987120418742242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 474/512 [01:32<00:07,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987088470754049\n",
      "0.9879005448555425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 481/512 [01:33<00:05,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9817037263506617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 483/512 [01:33<00:05,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9742324342816567\n",
      "0.9870152954454722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 493/512 [01:35<00:03,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9455252039109393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 497/512 [01:36<00:02,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8402829283938035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 500/512 [01:36<00:02,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9846299054950142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 502/512 [01:37<00:01,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9844248400897145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 504/512 [01:37<00:01,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825908144270673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 509/512 [01:38<00:00,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9867719735904442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:39<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8850084699216573\n",
      "0.9847029360588099\n",
      "[23, 41, 67, 75, 80, 102, 106, 134, 138, 142, 147, 150, 154, 159, 163, 169, 172, 176, 178, 179, 180, 187, 188, 190, 199, 201, 208, 214, 222, 225, 239, 246, 256, 264, 265, 267, 271, 272, 273, 274, 282, 288, 308, 310, 311, 315, 319, 321, 322, 325, 329, 340, 341, 343, 350, 352, 354, 356, 357, 359, 360, 369, 370, 372, 375, 377, 381, 383, 384, 386, 389, 395, 399, 402, 405, 407, 409, 410, 411, 415, 416, 418, 420, 422, 423, 426, 429, 430, 431, 432, 434, 437, 438, 440, 441, 442, 446, 452, 455, 456, 457, 458, 463, 466, 467, 468, 469, 472, 473, 479, 481, 482, 491, 495, 498, 500, 502, 507, 510, 511]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999844744604875, 0.999844744604875, 0.9999223723024375, 0.9999223723024375, 0.9999223723024375, 0.9999223723024375, 0.9999223723024375, 0.9999223723024375, 0.9999223723024375, 1.0, 0.9999223723024375, 0.9999223723024375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9991460953268125, 0.9987593068859143, 0.9987593068859143, 0.9980647077485942, 0.9980647077485942, 0.997602234903174, 0.9978263527843432, 0.9978263527843432, 0.9979053544234555, 0.9982894427705256, 0.9982894427705256, 0.9981341873754006, 0.998057246648613, 0.998057246648613, 0.9980626707309053, 0.9980626707309053, 0.9980626707309053, 0.9980626707309053, 0.9980626707309053, 0.9980626707309053, 0.9980626707309053, 0.9980626707309053, 0.9980626707309053, 0.9979830060156537, 0.9979830060156537, 0.9979830060156537, 0.9981389483815536, 0.9979830060156537, 0.9979046913473163, 0.9977507859991054, 0.9977507859991054, 0.9977507859991054, 0.9977507859991054, 0.9976731583015429, 0.9971317775416588, 0.9971317775416588, 0.9972100922099962, 0.9972100922099962, 0.9972100922099962, 0.9972100922099962, 0.9972863698606447, 0.9972863698606447, 0.9972863698606447, 0.9972863698606447, 0.9972863698606447, 0.9972863698606447, 0.998059970637077, 0.998059970637077, 0.9977535099875694, 0.9978297876382177, 0.9976738452723178, 0.9976738452723178, 0.9976738452723178, 0.9976738452723178, 0.9976738452723178, 0.9967469673088074, 0.9967469673088074, 0.9967469673088074, 0.9967469673088074, 0.9967443021299857, 0.9965903967817749, 0.9963595507067764, 0.996128017661003, 0.996128017661003, 0.996128017661003, 0.9962063323293404, 0.9962063323293404, 0.9959754274447, 0.9956676167482783, 0.9958235591141781, 0.9958235591141781, 0.9958985216329188, 0.9958985216329188, 0.9958985216329188, 0.9958985216329188, 0.9958985216329188, 0.9958985216329188, 0.9958978346621439, 0.9960503899634405, 0.9958181699468923, 0.9960497268873013, 0.9961280415556387, 0.9959059129028871, 0.9959835406004496, 0.9956086256490781, 0.9955309979515156, 0.9955309979515156, 0.9955309979515156, 0.9955309979515156, 0.9957611809503749, 0.9957611809503749, 0.9961413925380591, 0.9951550262542058, 0.9951550262542058, 0.9951550262542058, 0.9950729759410943, 0.9957585157715529, 0.9957585157715529, 0.9958361434691154, 0.9956873986276386, 0.9956873986276386, 0.9956873986276386, 0.9956873986276386, 0.9956873986276386, 0.9953813604464007, 0.9953813604464007, 0.9953813604464007, 0.9953076793382367, 0.9961354089309715, 0.9962130366285339, 0.9956123423309605, 0.9956123423309605, 0.9957634875287366, 0.9957634875287366, 0.9957588539685629, 0.9957588539685629, 0.9957588539685629, 0.9954490301490879, 0.9954490301490879, 0.9954490301490879, 0.9954490301490879, 0.9954490301490879, 0.9944524970035029, 0.9944524970035029, 0.9944524970035029, 0.9944524970035029, 0.9944524970035029, 0.9944524970035029, 0.9944524970035029, 0.9944524970035029, 0.9942947000471415, 0.9942947000471415, 0.9942947000471415, 0.9941407946989307, 0.9941407946989307, 0.9941407946989307, 0.9939902295529986, 0.9939158734213778, 0.9938382457238152, 0.9938382457238152, 0.9938382457238152, 0.9940732019961627, 0.9940732019961627, 0.9947570702792, 0.9947570702792, 0.9940791920519969, 0.9940791920519969, 0.9942287455416187, 0.9942287455416187, 0.9940761553253156, 0.9940761553253156, 0.9936879325662434, 0.9937623006451822, 0.9937623006451822, 0.9936879325662434, 0.9936109314628319, 0.9936109314628319, 0.9935327926697801, 0.9935327926697801, 0.9931523612233659, 0.9931523612233659, 0.9931523612233659, 0.9931523612233659, 0.9931523612233659, 0.9935281339233689, 0.9945980873446817, 0.9945257562834321, 0.9945257562834321, 0.9942954697332291, 0.9939200053927398, 0.9939200053927398, 0.9944487570559848, 0.9940666469169618, 0.9939862703872825, 0.9942158856016328, 0.9942158856016328, 0.9942158856016328, 0.9942158856016328, 0.9942158856016328, 0.9942158856016328, 0.9942158856016328, 0.9942158856016328, 0.9940627033861678, 0.9936014399473998, 0.9954390249603389, 0.9954390249603389, 0.9954390249603389, 0.9954390249603389, 0.9954390249603389, 0.9955166526579013, 0.9955166526579013, 0.9955166526579013, 0.9955166526579013, 0.9962889383024258, 0.9966008230342256, 0.9966008230342256, 0.9966008230342256, 0.9967507817930378, 0.9967507817930378, 0.9964470102169878, 0.9964470102169878, 0.9964470102169878, 0.9962187367898382, 0.9962187367898382, 0.9963798258166225, 0.9963798258166225, 0.9963798258166225, 0.9963798258166225, 0.996302848927864, 0.9962312658655803, 0.9962312658655803, 0.9960800256961913, 0.9960800256961913, 0.9960050631774506, 0.9959273976711773, 0.9959273976711773, 0.9950116765029567, 0.9950116765029567, 0.9950116765029567, 0.995170800560614, 0.9947076995540609, 0.9947076995540609, 0.9937824640582189, 0.9937824640582189, 0.9937824640582189, 0.9943938557170812, 0.9943148779726045, 0.9939171567239102, 0.9939171567239102, 0.9933918565215181, 0.9934655051553155, 0.9932384805605077, 0.9927818670870727, 0.9932446130565353, 0.9930920228402322, 0.9930920228402322, 0.9930072636743246, 0.9924716161654733, 0.9924716161654733, 0.9916161358933061, 0.9916161358933061, 0.9915692269560442, 0.9914871202074581, 0.9911157247046095, 0.9911157247046095, 0.9911967166513653, 0.991413551992831, 0.9913348059959959, 0.9908304825334151, 0.9902717218036786, 0.9901908487423824, 0.9901145591444163, 0.9901145591444163, 0.9895905401422775, 0.9895905401422775, 0.9886727919231413, 0.9886727919231413, 0.9886010006409106, 0.9877441669286762, 0.9877441669286762, 0.9886877968922531, 0.988547730897774, 0.988547730897774, 0.989018409673923, 0.9892469765866014, 0.9891720140678607, 0.9907617561050412, 0.9907617561050412, 0.9907617561050412, 0.9907617561050412, 0.9907617561050412, 0.9907617561050412, 0.9907617561050412, 0.9907617561050412, 0.9908345236641971, 0.9908345236641971, 0.9904528849736436, 0.9904528849736436, 0.9907294834116707, 0.9907294834116707, 0.9907294834116707, 0.9901401562093312, 0.9901401562093312, 0.9897063737211925, 0.9897063737211925, 0.9897063737211925, 0.9897063737211925, 0.9900204791444255, 0.9900204791444255, 0.9898639234348269, 0.9898639234348269, 0.9898639234348269, 0.989244653719706, 0.9897775677009559, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890089136506891, 0.9890893645172235, 0.9890893645172235, 0.9890893645172235, 0.9889351536440746, 0.9889351536440746, 0.9889351536440746, 0.9889351536440746, 0.9887179484874802, 0.9887179484874802, 0.9887179484874802, 0.9887179484874802, 0.9889209663869821, 0.9889209663869821]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Staged Training",
   "id": "fb240e46b7d021b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ecb18b7ab4aa9713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ee3467fa044e4264",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = torch.load('output_plus_and_minus_staged.pt')\n",
    "plt.plot(results['test_losses'])\n",
    "plt.plot(results['train_losses'])\n",
    "plt.yscale('log')\n",
    "\n"
   ],
   "id": "5aade8bb51301ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['test_losses'])\n",
    "    plt.plot(operation['train_losses'])\n",
    "plt.yscale('log')"
   ],
   "id": "28808178cbf3497e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3d93ed0b4e1192e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3aec01282fe76d92",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
