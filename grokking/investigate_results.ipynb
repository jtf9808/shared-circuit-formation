{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates[\"custom\"] = pio.templates[\"plotly\"]\n",
    "pio.templates[\"custom\"][\"layout\"][\"colorway\"] = px.colors.sequential.RdBu\n",
    "pio.templates.default = \"custom\"\n",
    "\n",
    "\n",
    "from matplotlib.pyplot import ylabel"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = torch.load('output.pt')\n",
    "plt.plot(results['train_losses'])\n",
    "plt.plot(results['test_losses'])\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'test_loss'])"
   ],
   "id": "30f6a8fe852173e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results.keys()",
   "id": "e795f0f65e0123a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = torch.load('output_plus_and_minus.pt', map_location=torch.device('cpu'))\n",
    "plt.plot(results['train_losses'])\n",
    "plt.plot(results['test_losses'])\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'test_loss',])\n"
   ],
   "id": "35c1b4b687dbae6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.keys()\n",
    "plt.plot(results['train_precision_scores'])\n",
    "plt.plot(results['test_precision_scores'])\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'])\n"
   ],
   "id": "c013eccf6bad7e41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['train_losses'])\n",
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['test_losses'])\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['+ train', '- train', '+ test', '- test'])\n"
   ],
   "id": "34920b03eb910477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['train_precisions'])\n",
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['test_precisions'])\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['+ train', '- train', '+ test', '- test'])\n",
    "plt.yscale('log')"
   ],
   "id": "b6ba1805198e6b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from transformer_lens import HookedTransformer, HookedTransformerConfig, HookedEncoderDecoder",
   "id": "a575feb809f2f098",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results['config'].device = 'cpu'",
   "id": "e386abc3edd174ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = HookedTransformer(results['config'])\n",
    "model.load_state_dict(results['model'])"
   ],
   "id": "d2b011f71015a369",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Show model works",
   "id": "35d73d3070993ae8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (1 + 15) % 113\n",
    "model(torch.tensor([1, 114, 15, 113]))[0,3].argmax().item()"
   ],
   "id": "375b5afe74bfbc6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (1 - 15) % 113\n",
    "model(torch.tensor([1, 115, 15, 113]))[0, 3].argmax().item()"
   ],
   "id": "e3a5396e38aefff3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run model on full dataset to look at activations/attention patterns",
   "id": "2d4be49803e03dfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import einops\n",
    "from modular_addition import ModularOperationsDataset\n",
    "operations = (lambda x, y: x + y, lambda x, y: x - y)\n",
    "dataset = ModularOperationsDataset(\n",
    "    base=113,\n",
    "    train_fraction=0.25,\n",
    "    operations=operations,\n",
    ")\n",
    "full_dataset = einops.rearrange(dataset.data, \"i j k -> (i k) j\")\n",
    "plus_dataset = dataset.data[:,:,0]\n",
    "minus_dataset = dataset.data[:,:,1]\n",
    "print(f\"plus: {plus_dataset.shape}, minus: {minus_dataset.shape}, full: {full_dataset.shape}\")"
   ],
   "id": "7aa162b9164db39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output, cache = model.run_with_cache(full_dataset)",
   "id": "4dbe355090135773",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "av_attention = cache[\"pattern\", 0].mean(dim=0).detach().cpu()",
   "id": "721fb5b0fa68f5b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i],\n",
    "        annot=True,            # Annotate each cell with its value\n",
    "        xticklabels=labels,    # Set x-axis labels\n",
    "        yticklabels=labels,    # Set y-axis labels\n",
    "        cmap=\"viridis\",        # Use a color map\n",
    "        cbar=False,            # Disable color bar to reduce clutter\n",
    "        ax=ax,                  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "    ax.set_ylabel('destination token', fontsize=12)\n",
    "    ax.set_xlabel('source token', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "8f476f9bd437b6c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot just attn where = is the destination (all we care about)\n",
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i, -1:],\n",
    "        annot=True,            # Annotate each cell with its value\n",
    "        xticklabels=labels,    # Set x-axis labels\n",
    "        yticklabels=[\"=\"],    # Set y-axis labels\n",
    "        cmap=\"viridis\",        # Use a color map\n",
    "        cbar=False,            # Disable color bar to reduce clutter\n",
    "        ax=ax                  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "    ax.set_ylabel('destination token', fontsize=12)\n",
    "    ax.set_xlabel('source token', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "db56b12a358369b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention patterns for +",
   "id": "791535f5a0b25288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_plus, cache_plus = model.run_with_cache(plus_dataset)\n",
    "av_attention = cache_plus[\"pattern\", 0].mean(dim=0).detach().cpu()\n",
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i],\n",
    "        annot=True,  # Annotate each cell with its value\n",
    "        xticklabels=labels,  # Set x-axis labels\n",
    "        yticklabels=labels,  # Set y-axis labels\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "b9e1a7348a0221f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention patterns for -",
   "id": "b1b2516931c8bf75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_minus, cache_minus = model.run_with_cache(minus_dataset)\n",
    "av_attention = cache_minus[\"pattern\", 0].mean(dim=0).detach().cpu()\n",
    "labels = ['a', 'operation', 'b', '=']\n",
    "# Create a figure to hold the 4 attention head plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"Attention Patterns for 4 Heads\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        av_attention[i],\n",
    "        annot=True,  # Annotate each cell with its value\n",
    "        xticklabels=labels,  # Set x-axis labels\n",
    "        yticklabels=labels,  # Set y-axis labels\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "50c6dfbe2391163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we plot the attention patterns for all combinations of a and b. Here we expect this to be very similar to what was found in the original paper with some potential interesting stuff on the openeration as the source attention.",
   "id": "f9d52a0bae2359bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Original model form paper\n",
    "import einops\n",
    "original_results = torch.load('output.pt')\n",
    "original_model = HookedTransformer(original_results['config'])\n",
    "original_model.load_state_dict(original_results['model'])\n",
    "a_s = einops.repeat(torch.arange(113), \"i -> (i j)\", j=113)\n",
    "b_s = einops.repeat(torch.arange(113), \"j -> (i j)\", i=113)\n",
    "equals = einops.repeat(\n",
    "    torch.tensor(113), \" -> (i j)\", i=113, j=113\n",
    ")\n",
    "original_dataset_correctly_ordered = torch.stack([a_s, b_s, equals], dim=1)\n",
    "original_output, original_cache = original_model.run_with_cache(original_dataset_correctly_ordered)\n",
    "a_to_equals_attn_patterns = original_cache[\"pattern\", 0][:,:,-1,0].detach().cpu()\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle(\"attention Patterns for a -> = for original model\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        a_to_equals_attn_patterns[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'Head {i + 1}', fontsize=12)\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "f9ff9c8ef356c0a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,0].detach().cpu()\n",
    "a_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,0].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns a -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        a_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        a_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "2f27ff9c1a131f73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "operation_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,1].detach().cpu()\n",
    "operation_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,1].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns operation -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        operation_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        operation_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "dd2046bd9111f825",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "b_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,2].detach().cpu()\n",
    "b_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,2].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns b -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        b_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        b_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "fcda120e716c6797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "equals_to_equals_attn_patterns_plus = cache_plus[\"pattern\", 0][:,:,-1,3].detach().cpu()\n",
    "equals_to_equals_attn_patterns_minus = cache_minus[\"pattern\", 0][:,:,-1,3].detach().cpu()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns = -> =\", fontsize=16)\n",
    "\n",
    "# Loop through each head and plot its attention pattern\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    sns.heatmap(\n",
    "        equals_to_equals_attn_patterns_plus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'+: Head {i + 1}', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "\n",
    "    sns.heatmap(\n",
    "        equals_to_equals_attn_patterns_minus[:, i].reshape(113, 113),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "        ax=ax  # Plot on the current axis\n",
    "    )\n",
    "    ax.set_title(f'-: Head {i + 1}', fontsize=12)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust to make space for the title\n",
    "plt.show()"
   ],
   "id": "fafbf0930ff3332f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definite periodicity but some interesting stuff happening. Maybe the model is learning a similar alg to the original but with more frequency components to accurately calculate both operations",
   "id": "f5e61090f74a9dec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check the embedding matrix",
   "id": "aac5d9ad89f86075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "W_E = model.embed.W_E.cpu().detach()[:113]\n",
    "sns.heatmap(\n",
    "        W_E.numpy(),\n",
    "        cmap=\"viridis\",  # Use a color map\n",
    "        cbar=False,  # Disable color bar to reduce clutter\n",
    "    )"
   ],
   "id": "b3800d4a2b965d67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "plt.plot(S)\n",
    "plt.title('W_E singular values')"
   ],
   "id": "bb7d3a00ea179c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Create heatmap\n",
    "px.imshow(U)"
   ],
   "id": "4d90564cc47521e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "px.line(U[:, :8].T, title=\"Principle Components of Embedding\").update_layout(\n",
    "    xaxis_title=\"Input Vocabulary\")"
   ],
   "id": "2e2d74c73b7bb9a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fourier_basis = []\n",
    "fourier_basis_names = []\n",
    "fourier_basis.append(torch.ones(113))\n",
    "fourier_basis_names.append('Constant')\n",
    "for freq in range(1, 113//2 + 1):\n",
    "    fourier_basis.append(torch.sin(torch.arange(113)*2 * torch.pi*freq /113))\n",
    "    fourier_basis_names.append(f'Sin {freq}')\n",
    "    fourier_basis.append(torch.cos(torch.arange(113)*2 * torch.pi*freq /113))\n",
    "    fourier_basis_names.append(f'Cos {freq}')\n",
    "fourier_basis = torch.stack(fourier_basis, dim=0)\n",
    "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1)\n",
    "px.imshow(fourier_basis, y=fourier_basis_names,color_continuous_scale='RdBu',).update_layout(xaxis_title=\"Input\", yaxis_title=\"Cofourier_basis_namesmponent\")"
   ],
   "id": "d4a44fdcaf8627b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ fourier_basis.T)",
   "id": "427ff60aa421b7fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ W_E, color_continuous_scale='RdBu', y =fourier_basis_names, title='embedding_in_fourier_basis').update_layout(xaxis_title=\"Residual Stream\", yaxis_title=\"fourier_component\")",
   "id": "3d71ff25c8a80e4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(y=(fourier_basis @ W_E).norm(dim=-1), x=fourier_basis_names, title='embedding_in_fourier_basis').update_layout(xaxis_title=\"Residual Stream\", yaxis_title=\"fourier_component\")",
   "id": "f20289976803ac1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(fourier_basis @ W_E).norm(dim=-1)",
   "id": "55c8b26490695f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(fourier_basis[[32, 48, 100]].mean(0))\n",
   "id": "a23d7e04e6e4f002",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "key_freq_indicies = [31, 32, 47, 48, 99, 100]\n",
    "key_fourier_embed = (fourier_basis @ W_E)[key_freq_indicies]\n",
    "px.imshow(key_fourier_embed@key_fourier_embed.T,color_continuous_scale='RdBu',color_continuous_midpoint=0)"
   ],
   "id": "167e6e7f05d4afe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Look at frequencies in the mlp hidden activations",
   "id": "3fac27929ca8ecf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neuron_acts_plus = cache_plus[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_pre_acts_plus = cache_plus[\"pre\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_acts_minus = cache_minus[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_pre_acts_minus = cache_minus[\"pre\", 0, \"mlp\"][:, -1, :]"
   ],
   "id": "3200d749ead85d16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "neuron_acts_plus.shape",
   "id": "4b119c62a1c2c388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_plus[:, 1].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "3bcd7abc3240fbdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_minus[:, 1].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "11cf87ecb59bd0fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ neuron_acts_plus[:, 1].reshape(113, 113) @ fourier_basis.T, color_continuous_scale='RdBu', color_continuous_midpoint=0, title='2d transform of neuron 1 for plus op', x=fourier_basis_names, y=fourier_basis_names).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "6e23434d5b0decc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis @ neuron_acts_minus[:, 1].reshape(113, 113) @ fourier_basis.T, color_continuous_scale='RdBu', title='2d transform of neuron 1 for minus op', x=fourier_basis_names, y=fourier_basis_names, color_continuous_midpoint=0).update_layout(xaxis_title=\"b\", yaxis_title=\"a\")\n",
   "id": "8aa38d8e77acf782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### neuron clusters",
   "id": "d8885832cb151a3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fourier_neuron_acts_plus = fourier_basis @ einops.rearrange(neuron_acts_plus, \"(a b) neuron -> neuron a b\", a=113, b=113) @ fourier_basis.T\n",
    "# Center these by removing the mean - doesn't matter!\n",
    "fourier_neuron_acts_plus[:, 0, 0] = 0.\n",
    "print(\"fourier_neuron_acts\", fourier_neuron_acts_plus.shape)"
   ],
   "id": "30ae026847c114d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neuron_freq_norm_plus = torch.zeros(113//2, model.cfg.d_mlp)\n",
    "for freq in range(0, 113//2):\n",
    "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "            neuron_freq_norm_plus[freq] += fourier_neuron_acts_plus[:, x, y]**2\n",
    "neuron_freq_norm_plus = neuron_freq_norm_plus / fourier_neuron_acts_plus.pow(2).sum(dim=[-1, -2])[None, :]\n",
    "px.imshow(neuron_freq_norm_plus, y=torch.arange(1, 113//2+1), title=\"Neuron Frac Explained by Freq plus\", color_continuous_scale='RdBu', color_continuous_midpoint=0, aspect=\"auto\",).update_layout(xaxis_title=\"Neuron\", yaxis_title=\"Freq\")"
   ],
   "id": "f89486ebc9c69b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(neuron_freq_norm_plus.max(dim=0).values.sort().values, title=\"Max Neuron Frac Explained over Freqs plus\").update_layout(xaxis_title=\"Neuron\")",
   "id": "ed10053f7805191c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fourier_neuron_acts_minus = fourier_basis @ einops.rearrange(neuron_acts_minus, \"(a b) neuron -> neuron a b\", a=113, b=113) @ fourier_basis.T\n",
    "# Center these by removing the mean - doesn't matter!\n",
    "fourier_neuron_acts_minus[:, 0, 0] = 0.\n",
    "print(\"fourier_neuron_acts\", fourier_neuron_acts_minus.shape)"
   ],
   "id": "22c5d09f580817d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neuron_freq_norm_minus = torch.zeros(113//2, model.cfg.d_mlp)\n",
    "for freq in range(0, 113//2):\n",
    "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "            neuron_freq_norm_minus[freq] += fourier_neuron_acts_minus[:, x, y]**2\n",
    "neuron_freq_norm_minus = neuron_freq_norm_minus / fourier_neuron_acts_minus.pow(2).sum(dim=[-1, -2])[None, :]\n",
    "px.imshow(neuron_freq_norm_minus, y=torch.arange(1, 113//2+1), title=\"Neuron Frac Explained by Freq plus\", color_continuous_scale='RdBu', color_continuous_midpoint=0, aspect=\"auto\",).update_layout(xaxis_title=\"Neuron\", yaxis_title=\"Freq\")"
   ],
   "id": "3b29a6632176c08d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(neuron_freq_norm_minus.max(dim=0).values.sort().values, title=\"Max Neuron Frac Explained over Freqs plus\").update_layout(xaxis_title=\"Neuron\")\n",
   "id": "de5a6c1abde0f88a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TODO: Maybe try to combine the above?",
   "id": "fcff5a73bc45f7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Look at Unembedding",
   "id": "12353d42bebd046d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "W_U = model.unembed.W_U.cpu().detach()\n",
    "W_O = model.blocks[0].mlp.W_out.cpu().detach()"
   ],
   "id": "c970ccb5012244c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "20b0e814ba8849f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "W_OU = W_O @ W_U",
   "id": "93a0bd220dcb1168",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(W_OU, color_continuous_scale='RdBu', color_continuous_midpoint=0, aspect=\"auto\", height=800, width=800)",
   "id": "1646fd459934907",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(fourier_basis@W_OU.T, y=fourier_basis_names,color_continuous_scale='RdBu', color_continuous_midpoint=0, title=\"frequency componenets of W_OU\").update_layout(xaxis_title=\"output\", yaxis_title=\"Freq\")",
   "id": "e7d0d53c8a3f71f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.line(y=(fourier_basis@W_OU.T).T.norm(dim=0), x=fourier_basis_names,title=\"normed frequency componenets of W_OU\").update_layout(xaxis_title=\"output\", yaxis_title=\"Freq\")",
   "id": "51ae72dff1208312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "## show that corresponding the frequency component of W_OU output for the neuron activations with a frequency 16 is also 16 (this is for the final cos(w) output in the the trig addition)",
   "id": "b73c21947d4c3904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neurons_16 = neuron_freq_norm_plus[16-1] > 0.7\n",
    "px.line(y=(fourier_basis@W_OU[neurons_16].T).T.norm(dim=0), x=fourier_basis_names,title=\"normed frequency componenets of W_OU\").update_layout(xaxis_title=\"output\", yaxis_title=\"Freq\")\n"
   ],
   "id": "f3e53319d738759b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Looking at plus and minus",
   "id": "3edb449142e38bb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_plus[:, 2].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0, title = 'plus operation neuron activations [neuron 2]').update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "956b7ac10b2b397c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.imshow(neuron_acts_minus[:, 2].reshape(113, 113), color_continuous_scale='RdBu', color_continuous_midpoint=0, title = 'minus operation neuron activations [neuron 2]').update_layout(xaxis_title=\"b\", yaxis_title=\"a\")",
   "id": "9ed9b9ca52ce6f45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3adc97dc04374d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.subplots as sp\n",
    "import numpy as np\n",
    "max_plots = 25\n",
    "\n",
    "num_plots = min(max_plots, neuron_acts_plus.shape[1])\n",
    "grid_size = int(2)\n",
    "\n",
    "fig = sp.make_subplots(rows=num_plots, cols=2, subplot_titles=[j for i in range(num_plots) for j in [f\"Plus Neuron {i}\", f\"Minus Neuron {i}\"]])\n",
    "\n",
    "for i in range(num_plots):\n",
    "    reshaped_data_plus = neuron_acts_plus[:, i].reshape(113, 113)\n",
    "    max_val_plus = reshaped_data_plus.max()\n",
    "    reshaped_data_minus = neuron_acts_minus[:, i].reshape(113, 113)\n",
    "    max_val_minus = reshaped_data_minus.max()\n",
    "    overall_max = max([max_val_plus, max_val_minus])\n",
    "    normalized_data_plus = reshaped_data_plus / (overall_max + 1e-8)\n",
    "    normalized_data_minus = reshaped_data_minus / (overall_max + 1e-8)\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_plus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=1)\n",
    "\n",
    "\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_minus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500 * max_plots, width=grid_size * 500,\n",
    "    title_text=\"Neuron Activations\",\n",
    "    coloraxis=dict(cmid=0, colorscale='RdBu')\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ],
   "id": "18a655b52992df95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5e743fae39a61df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plus_not_minus = []\n",
    "minus_not_plus = []\n",
    "for i in range(512):\n",
    "    if neuron_acts_minus[:, i].sum()/neuron_acts_plus[:, i].sum() < 0.1:\n",
    "        plus_not_minus.append(i)\n",
    "    if neuron_acts_plus[:, i].sum()/neuron_acts_minus[:, i].sum() < 0.1:\n",
    "        minus_not_plus.append(i)\n",
    "\n",
    "\n",
    "all_ = plus_not_minus + minus_not_plus\n",
    "num_plots = len(all_)\n",
    "grid_size = int(2)\n",
    "\n",
    "fig = sp.make_subplots(rows=num_plots, cols=2, subplot_titles=[j for i in range(num_plots) for j in [f\"Plus Neuron {i}\", f\"Minus Neuron {i}\"]])\n",
    "\n",
    "for i, neuron_index in enumerate(all_):\n",
    "    reshaped_data_plus = neuron_acts_plus[:, neuron_index].reshape(113, 113)\n",
    "    max_val_plus = reshaped_data_plus.max()\n",
    "    reshaped_data_minus = neuron_acts_minus[:, neuron_index].reshape(113, 113)\n",
    "    max_val_minus = reshaped_data_minus.max()\n",
    "    overall_max = max([max_val_plus, max_val_minus])\n",
    "    normalized_data_plus = reshaped_data_plus / (overall_max + 1e-8)\n",
    "    normalized_data_minus = reshaped_data_minus / (overall_max + 1e-8)\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_plus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=1)\n",
    "\n",
    "\n",
    "    heatmap = px.imshow(\n",
    "        normalized_data_minus,\n",
    "    )\n",
    "    for trace in heatmap.data:\n",
    "        fig.add_trace(trace, row=i+1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500 * max_plots, width=grid_size * 500,\n",
    "    title_text=\"Neuron Activations\",\n",
    "    coloraxis=dict(cmid=0, colorscale='RdBu')\n",
    ")\n",
    "fig.show()"
   ],
   "id": "7e7e76e44c5e3a7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ablating neurons that only fire for one operation",
   "id": "6fff5dfe48dc6d9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:20:03.069527Z",
     "start_time": "2025-03-17T09:20:01.746642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_lens import utils\n",
    "def plus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, plus_not_minus] = 0.\n",
    "    return value\n",
    "\n",
    "plus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "plus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "def minus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    value[:, :, minus_not_plus] = 0.\n",
    "    return value\n",
    "\n",
    "minus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "minus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )"
   ],
   "id": "227081cfd328d443",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the value tensor: torch.Size([12769, 4, 512])\n",
      "Shape of the value tensor: torch.Size([12769, 4, 512])\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:20:03.112203Z",
     "start_time": "2025-03-17T09:20:03.078812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score\n",
    "plus_dataset_labels = (plus_dataset[:,0] + plus_dataset[:,2]) % 113\n",
    "minus_dataset_labels = (minus_dataset[:,0] - minus_dataset[:,2]) % 113\n",
    "\n",
    "plus_neurons_ablated_ablated_plus_predictions = plus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "plus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, plus_neurons_ablated_ablated_plus_predictions, average='macro')\n",
    "\n",
    "plus_neurons_ablated_ablated_minus_predictions = plus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "plus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, plus_neurons_ablated_ablated_minus_predictions, average='macro')\n",
    "\n",
    "minus_neurons_ablated_ablated_plus_predictions = minus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "minus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, minus_neurons_ablated_ablated_plus_predictions, average='macro')\n",
    "\n",
    "minus_neurons_ablated_ablated_minus_predictions = minus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "minus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, minus_neurons_ablated_ablated_minus_predictions, average='macro')\n",
    "\n",
    "\n",
    "print(f'after ablating all but neurons only active in plus: plus precision {plus_neurons_ablated_ablated_plus_precision} minus precision {plus_neurons_ablated_ablated_minus_precision}')\n",
    "print(f'after ablating all but neurons only active in minus: plus precision {minus_neurons_ablated_ablated_plus_precision} minus precision {minus_neurons_ablated_ablated_minus_precision}')"
   ],
   "id": "98182ff86cef6a50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after ablating all but neurons only active in plus: plus precision 0.22997348393722042 minus precision 0.9917927362577618\n",
      "after ablating all but neurons only active in minus: plus precision 0.9990725177699925 minus precision 0.3939233817539317\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:22:14.241018Z",
     "start_time": "2025-03-17T09:22:12.486931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def all_but_plus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    value[:, :, [i for i in range(512) if i not in plus_not_minus]] = 0.\n",
    "    return value\n",
    "\n",
    "all_but_plus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "all_but_plus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_plus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "def all_but_minus_mlp_neuron_ablation_hook(\n",
    "    value,\n",
    "    hook\n",
    "):\n",
    "    value[:, :, [i for i in range(512) if i not in minus_not_plus]] = 0.\n",
    "    return value\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_plus_results = model.run_with_hooks(\n",
    "    plus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_minus_results = model.run_with_hooks(\n",
    "    minus_dataset,\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"post\", 0, \"mlp\"),\n",
    "        all_but_minus_mlp_neuron_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n"
   ],
   "id": "96eba0d148ec978a",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:22:30.327464Z",
     "start_time": "2025-03-17T09:22:30.296530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_but_plus_neurons_ablated_ablated_plus_predictions = all_but_plus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_plus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, all_but_plus_neurons_ablated_ablated_plus_predictions, average='macro')\n",
    "\n",
    "all_but_plus_neurons_ablated_ablated_minus_predictions = all_but_plus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_plus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, all_but_plus_neurons_ablated_ablated_minus_predictions, average='macro')\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_plus_predictions = all_but_minus_neurons_ablated_ablated_plus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_minus_neurons_ablated_ablated_plus_precision = precision_score(plus_dataset_labels, all_but_minus_neurons_ablated_ablated_plus_predictions, average='macro')\n",
    "\n",
    "all_but_minus_neurons_ablated_ablated_minus_predictions = all_but_minus_neurons_ablated_ablated_minus_results[:,-1,:].argmax(dim=-1).cpu().numpy()\n",
    "all_but_minus_neurons_ablated_ablated_minus_precision = precision_score(minus_dataset_labels, all_but_minus_neurons_ablated_ablated_minus_predictions, average='macro')\n",
    "\n",
    "\n",
    "print(f'after ablating all but neurons only active in plus: plus precision {all_but_plus_neurons_ablated_ablated_plus_precision} minus precision {all_but_plus_neurons_ablated_ablated_minus_precision}')\n",
    "print(f'after ablating all but neurons only active in minus: plus precision {all_but_minus_neurons_ablated_ablated_plus_precision} minus precision {all_but_minus_neurons_ablated_ablated_minus_precision}')"
   ],
   "id": "97c87c741d138ef4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after ablating all but neurons only active in plus: plus precision 0.00806736360449042 minus precision 0.0005453249533054084\n",
      "after ablating all but neurons only active in minus: plus precision 0.0011633032147423442 minus precision 0.014751908804845129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackfulls/Incyan/shared-circuit-formation/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/jackfulls/Incyan/shared-circuit-formation/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/jackfulls/Incyan/shared-circuit-formation/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/jackfulls/Incyan/shared-circuit-formation/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# do some recursive albation studies",
   "id": "408abaa9f3243e45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Staged Training",
   "id": "fb240e46b7d021b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ecb18b7ab4aa9713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ee3467fa044e4264",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = torch.load('output_plus_and_minus_staged.pt')\n",
    "plt.plot(results['test_losses'])\n",
    "plt.plot(results['train_losses'])\n",
    "plt.yscale('log')\n",
    "\n"
   ],
   "id": "5aade8bb51301ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for operation in results['operations_losses']:\n",
    "    plt.plot(operation['test_losses'])\n",
    "    plt.plot(operation['train_losses'])\n",
    "plt.yscale('log')"
   ],
   "id": "28808178cbf3497e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3d93ed0b4e1192e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3aec01282fe76d92",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
